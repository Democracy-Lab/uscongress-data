{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open web page\n",
    "\n",
    "# while link anchor does not have 1994:\n",
    "### scrape the link\n",
    "### if not every link has 1994 then increment and go to next page. \n",
    "### if every link has 1994 end scraping. \n",
    "\n",
    "\n",
    "# https://www.congress.gov/search?q=\n",
    "\n",
    "# https://www.congress.gov/search?q=%7B%22source%22%3A%22congrecord%22%2C%22search%22%3A%22war%22%7D&pageSize=100&page=1\n",
    "# https://www.congress.gov/search?q=%7B%22source%22%3A%22congrecord%22%2C%22search%22%3A%22change%22%7D&pageSize=100&page=1\n",
    "\n",
    "#https://www.congress.gov/search?q=%7B%22source%22%3A%22congrecord%22%2C%22search%22%3A%22climate%20change%22%7D\n",
    "#https://www.congress.gov/search?q=%7B%22source%22%3A%22congrecord%22%2C%22search%22%3A%22natural%20gas%22%7D\n",
    "\n",
    "\n",
    "#if year is not 1994, then increment (go to page 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_records(url):\n",
    "    daily_editions = re.compile(r'20.*/|199[5-9]/')\n",
    "    bound_editions = re.compile(r'/1990/|/1991/|/1992/|/1993/|/1994/')\n",
    "\n",
    "    bound_edition_hits = 0\n",
    "    page = 12\n",
    "\n",
    "    urls = {}\n",
    "    \n",
    "    while bound_edition_hits < 100:\n",
    "        page = page + 1\n",
    "\n",
    "        #print('page' + str(page))\n",
    "\n",
    "        request = requests.get(url + str(page))\n",
    "        current_page = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        all_links_current_page = current_page.find_all('a')\n",
    "\n",
    "        for link in all_links_current_page:\n",
    "            if 'congressional-record' in link.get('href'):\n",
    "                \n",
    "                if re.search(bound_editions, link.get('href')):\n",
    "                    bound_edition_hits = bound_edition_hits + 1\n",
    "\n",
    "                elif re.search(daily_editions, link.get('href')):\n",
    "                    result = re.search(r'(?<=\").*(?=\")', str(link))\n",
    "                    urls['https://www.congress.gov' + str(result.group(0))] = 'url'\n",
    "                    bound_edition_hits = 0\n",
    "\n",
    "        #print('beh' + str(bound_edition_hits))\n",
    "\n",
    "        time.sleep(.5)\n",
    "\n",
    "    return(urls)\n",
    "\n",
    "def scrape_records(all_urls):\n",
    "    match_target_text = re.compile(r'(?<=From the Congressional Record Online through the Government Publishing Office \\[www.gpo.gov\\] ).*(?=____________________)', re.MULTILINE)\n",
    "\n",
    "    target_text = {}\n",
    "\n",
    "    for url in all_urls.keys():\n",
    "        request = requests.get(url)\n",
    "        current_page = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "        untagged_text = current_page.body.find_all(text=True)\n",
    "\n",
    "        for text in untagged_text:\n",
    "            text = text.replace('\\n',' ')\n",
    "            text = \" \".join(text.split())\n",
    "\n",
    "            try:\n",
    "                matched = re.search(match_target_text, text).group(0)\n",
    "                target_text[matched] = url\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        time.sleep(.5)\n",
    "    \n",
    "    return(target_text)\n",
    "\n",
    "def format_records(record):\n",
    "    df = pd.DataFrame(record.items(), columns=['raw', 'url'])\n",
    "\n",
    "    df['title'] = df['raw'].str.extract(r'^(?=.+[a-z])([A-Z\\W]+\\b)')\n",
    "\n",
    "    df['title'] = df['title'].str.strip()\n",
    "\n",
    "    df['body'] = df['raw'].str.replace(r'^(?=.+[a-z])([A-Z\\W]+\\b)', '', regex=True)\n",
    "\n",
    "    df['split'] = df['body'].str.split(r'([A-Z][\\w]+.\\s?[A-Z]+\\b.)')\n",
    "\n",
    "    df['split'] = df['split'].apply(lambda row: [x for x in row if x])\n",
    "\n",
    "    # List items 0, 2, 4, ... are speaker names\n",
    "    df['speaker_name'] = df['split'].str[::2]\n",
    "    # List items 1, 3, 5, ... are spoken text\n",
    "    df['speaker_text'] = df['split'].str[1::2]\n",
    "\n",
    "    res = df[['url', 'title', 'speaker_name', 'speaker_text']].copy()\n",
    "\n",
    "    res = res.fillna('')\n",
    "\n",
    "    #res = res.explode(['speaker_name', 'speaker_text'])\n",
    "    res = res.copy().explode('speaker_name', ignore_index = True)\n",
    "    res = res.copy().explode('speaker_text', ignore_index = True)\n",
    "\n",
    "    #res['speaker_name'] = res['speaker_name'].str.rstrip('.')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = search_records('https://www.congress.gov/search?q=%7B%22source%22%3A%22congrecord%22%2C%22search%22%3A%22croatian%22%7D&pageSize=100&page=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = scrape_records(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3 = format_records(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        search_term = 'croatian'\n",
    "        #search_term = sys.argv[1]\n",
    "        #first_year = sys.argv[2]\n",
    "        #last_year = sys.argv[3]\n",
    "    except IndexError:\n",
    "        exit('Congressional Data Scraper takes three arguments: search term, first year, last year. Please see github.com/stephbuon/congressional-data-scraper for more information')\n",
    "\n",
    "    #output_folder = base_folder + '/tei_output'\n",
    "\n",
    "    #if not os.path.exists(output_folder):\n",
    "    #    os.mkdir(output_folder)\n",
    "\n",
    "    searched = search_records('https://www.congress.gov/search?q=%7B%22source%22%3A%22congrecord%22%2C%22search%22%3A%22' + search_term + '%22%7D&pageSize=100&page=')\n",
    "    scraped = scrape_records(searched)\n",
    "    formatted = format_records(scraped)\n",
    "\n",
    "    formatted.to_csv('/home/stephbuon/projects/congressional-data-scraper/congressional_records_keyword_' + search_term + '.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
